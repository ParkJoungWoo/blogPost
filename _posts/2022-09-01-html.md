---
layout: post
title: "내가 보려고 정리한 크롤링"
subtitle: 데이터 분석
categories: project
tags: [data analysis]
---

# 크롤링

![gg](/assets/img/0901/ggobuk.png)
### 크롤링의 정의 : 크롤링은 데이터를 수집하고 분류하는것을 의미

data lake, data dam : 데이터를 저장하기 위한 공간

    정형데이터 : 
    비정형데이터 :

    일반적인 크롤링 순서

        1. html 문서를 다운로드 (url지정)
        2. 파싱(BeautifulSoup)
        3. 파싱된 문서에서 원하는 데이터 위치를 검색(시간이 걸릴 수 있음)
        4. 자료를 구조화 => pandas
        5. 파일 저장
### 파싱은 웹페이지의 데이터를 문법에 맞게 분석해 내는 것

- 사용자가 원하는 형태로 추출해서 사용하기
- 사이트, 회사에서 제공하는 API를 사용하거나 사용자가 직접 코드를 만든다.
    - API는 정제된 데이터를 이용가능하며 윤리적 • 법적 문제가 없음



## HTTP Method

Client가 GET, POST를 사용하여 Server(웹서버)에 정보 요청

    GET : URL에 값을 추가하여 전달.
    POST : HTTP 프로토콜 Hearder에 값을 추가하여 전달.

python에서는 Request 모듈을 통해 데이터를 다운받을 수 있다.
```python
import urllib.request as request
#파이썬 홈페이지 html 다운받기
url = 'http://python.org/'
local_filename, headers = request.urlretrieve(url, 'python.html')
html = open(local_filename)
print(local_filename, headers, sep='\n\n')
html.close()
```
python.html 파일을 열어보면
![img](/assets/img/0901/domain.png)

이미지, 비디오 같은 리소스파일은 없기 때문에 글자만 나타나게 된다.

## 파일을 여는 방법들

자주 쓰이는 "한줄씩 파일을 읽어 리스트로 저장" 에 대한 방법을 알아보자.

파일은 두가지 종류로 구분된다

```
t - text : 메모장으로 열기, 가능 
    ex) txt, csv, html, htm
b - binary : 해당 파일을 읽고/쓰기 위한 별도 프로그램이 필요
    ex) jpg, png, mp3, xlsx
```
- read 사용하기
```python
html = open(local_filename, 'rt', encoding='UTF8')
#read 메소드를 사용한다.
content1 = html.read()
html.close()
print(html, type(content1), content1[:300], sep='\n\n')
```
- readlines 사용하기
```python
with open(local_filename, 'rt', encoding='UTF8') as f:
    content2 = f.readlines()
print(type(content2), content2[:3], sep='\n\n') 
```
- 바이너리 데이터 저장하기
```python
url = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS6tHOuYhEUtyESQ4Ojcx4w4ekj72Dzj6DYpw&usqp=CAU"
savename = "GGOBOOK.png"
data = request.urlopen(url).read()
with open(savename, mode='wb') as t:
    t.write(data)
#꼬북파일에 꼬북이 이미지가 저장되어있다.
```
### 웹 데이터 추출

url에 추가할 데이터 만들기

URL에 매개 변수 추가하는 방법

    1. 매개변수 생성 => params 생성
        - values : dict로 작성
        - params = urllib.parse.urlencode(values)   
    2. 매개변수(params) 추가 방법
        - url_params = url + '?' + params

```python
#dictionary로 만든다
url = 'https://news.naver.com/main/main.nhn'
values = {'mode':'LSD', 'mid':'shm','sid1:103'}
params = urllib.parse.urlencode(values)  
#mode=LSD&mid=shm&sid1=103
url_params = url + "?" + params
display(url_params)
```
url으로 데이터를 읽어 오려면?
```python
url = 'https://finance.naver.com/sise/sise_index_day.nhn'
values = {'code':'KOSPI','page':1  } 
params = urllib.parse.urlencode(values)
url_params = url + "?" + params
#parse를 통해 url_params 만들기

#request urlopen으로 받아와서 read()를 통해 데이터를 읽기
data = urllib.request.urlopen(url_params).read()
#parse unquote를 통해 인코딩으로 데이터를 쓰기
html = urllib.parse.unquote(data,encoding="cp949") #cp949
print(html)
```
### url로 데이터 읽기 심화
```python
def get_html(url, values):
    import urllib.request
    import urllib.parse
    import chardet

    params = urllib.parse.urlencode(values)
    url_params = url + "?" + params
    data = urllib.request.urlopen(url_params).read()
    #data = 통신용 (bytes 타입)
    enc = chardet.detect(data)['encoding']
    #encoding 수행한 데이터
    return data.decode(enc)

url = 'https://finance.naver.com/sise/sise_index_day.nhn'
values = {'code':'KOSPI', 'page':1 }
html = get_html(url, values)
html[:200]
```
## Beautiful Soup

> Beautiful Soup is a Python library for pulling data out of HTML and XML files.

- 사용법
```python
from bs4 import BeautifulSoup

html = """<html>
<body>
   <h1>크롤링</h1>
   <p>웹 페이지 분석</p>
   <p>원하는 내용 추출</p>
</body>
</html>"""

soup = BeautifulSoup(html, 'html.parser')
print(type(soup), soup, sep='\n\n')
```
출력
```
<class 'bs4.BeautifulSoup'>

<html>
<body>
<h1>크롤링</h1>
<p>웹 페이지 분석</p>
<p>원하는 내용 추출</p>
</body>
</html>
```
이와 같이 html 파일을 읽어 변환해주는 것이 B-S의 기능이다.

### find 함수의 사용
- soup.find('태그명')
- soup.find(id='id명')
- soup.find(text='텍스트내용')
- soup.find('태그명', attrs={'id':'...', 'class':'...'})

```python
from bs4 import BeautifulSoup

html2 = """<html>
<body>
   <h1 id='title'>크롤링</h1>
   <p id='p1'>웹 페이지 분석</p>
   <p id='p2'>원하는 내용 추출</p>
</body>
</html>"""
soup2 = BeautifulSoup(html2, 'html.parser')
h1 = soup2.html.h1
h1 = soup2.find('h1')
h1 = soup2.find(id='title')
h1 = soup2.find(text='크롤링')
p2 = soup2.find('p', {"id":'p2'})
```

여러개의 요소에서 찾기
> find_all
```python
# 실습을 위한 html4, soup4 생성 
html4 = """<html>
<body>
   <h1 id='title'>크롤링</h1>
   <p id='body'>웹 페이지 분석</p>
   <p class='A B C'>원하는 내용 추출</p>
</body>
</html>"""
soup4 = BeautifulSoup(html4, 'html.parser')

link = soup4.find_all('p')
#link -> list
for a in link:
# a.attrs -> dict
# a.attrs는 dict이기 때문에 get 메소드를 사용할 수 있다.
    print(a.attrs)
    print(a.attrs.get('id',False))
```
출력
```
{'id': 'body'}
body
{'class': ['A', 'B', 'C']}
False
```
soup에서 find all한것은 dictionary의 list이다.

### select 함수의 이용

CSS를 선택하기 위한 메소드이다.

    - select_one(<선택자>) : CSS 선택자로 요소 하나를 추출
    - select(<선택자>) : CSS 선택자로 요소 여러 개를 리스트로 추출
    - 선택자 기호
    - '#' : id  (유일한 별명)
    - '.' : class (유일하지 않을 수 있음)
    - '>' : 하위 태그
```python
html="""<html><body>
<div id="loc">
    <h1>지역</h1>
    <ul class="items">
        <li>서울></li>
        <li>부산></li>
        <li>대구></li>
    </ul>
</div>
</body></html>"""
soup = BeautifulSoup(html, 'html.parser')

# [10] select_one() 사용, <h1>지역</h1> 가져오기
# id접근은 #, class 접근은 .  하위는 >
h1 = soup.select_one('div#loc > h1')

# [11] select_one() 사용, <li>서울></li> 가져오기
#seoul = soup.select_one('li') - 너무 느슨한 li 찾기
seoul = soup.select_one("div#loc > ul.items > li")

# [12] select() 사용, 모든 li 가져오기
li_list = soup.select('div#loc > ul.items > li') #좁게 찾기

# [13] li_list에서 텍스트 정보만 꺼내기
print([i.string[:-1] for i in li_list])
#comprehension 쓰기
```

