---
layout: post
title: "머신러닝 평가지표"
subtitle: 머신러닝
categories: machine learning
tags: [ml]
---
# 평가 지표

## 분류 평가 지표

1. 정확도 Accuracy

    (predict==y).mean()
2. Confusion Matrix (Recall, Precision)

## 회귀 평가 지표
> 회귀분석의 평가는 분류보다 다소 난해할 수 있다.

ex) 연속선상의 숫자를 예측하기/ 아파트 집값 예측과 배추값 예측

1억 vs 1억 5천만 / 5천원 vs 8천원 -> 값의 차이는 크나 평가는 달라짐(전자가 우수한 예측)


|평가지표||||
|---|---|---|---|
|Linear Regression|||
|R2(R-Squared)|MAE|MSE|RMSE|
|||||

## variance vs bias

![img](/assets/img/0920/want.png)

위에서 설명한것과 같이 해결방법은 아래와 같다.

1. 이상치
2. Polynominal Feature
3. Regulization

### Regulization and PipeLine

그 중에서 정규화에 대해 설명하자면

전체적인 columns에 적용하고 Data의 Variance를 낮추기 위해서(OverFitting을 해결하기 위하여) Regularization을 진행한다.

|L1 규제|L2 규제|
|---|---|
|Lasso|Ridge|
|MSE + λ * ∑ \|w\| |MSE + λ * ∑ w²|

    규제 -> w의 값을 규제한다.
    Q. 규제강도(λ)를 0으로 하면?
        A. 단순 Linear Regression과 동일

    Q. 규제 강도를 많이 준다?
        A. high Bias 발생, 학습을 더 많이 해야한다.

    1. Lasso가 가장 낮은 MSE를 보여준다
    2. alpha = 100이면 큰 에러를 보여준다. == 안정성이 낮다
    3. 기업에서는 많이  사용하지 않는다.

    규제 강도가 클 수록 모델이 성능이 많이 떨어진다

    Lasso는 규제를 조금만 강하게 부여하면 불안정함을 보인다. 
    -> 현업에서는 Ridge Model을 선호한다.
    -> 특정한 컬럼에 극단으로 치우치는 경향을 보인다.
    
```python
from sklearn.linear_model import Ridge, Lasso

# 모델 정의
lasso_100 = Lasso(alpha=100)
# 학습
lasso_100.fit(x_train, y_train)
# 예측
lasso_pred_100 = lasso_100.predict(x_test)

# 모델 정의
ridge_100 = Ridge(alpha=100)
# 학습
ridge_100.fit(x_train, y_train)
# 예측
ridge_pred_100 = ridge_100.predict(x_test)
```


#### Elastic Net
> MSE + λ*L₁ + (1-λ)*L₂

라쏘와 릿지의 비율을 지정해서 혼합한 규제

```python
# 엘라스틱 비율 지정하기
# L1 위주
elsticnet_80 = ElasticNet(alpha=5, l1_ratio=0.8)
elsticnet_80.fit(x_train, y_train)
elasticnet_pred_80 = elsticnet_80.predict(x_test)

# L2위주로
elsticnet_20 = ElasticNet(alpha=5, l1_ratio=0.2)
elsticnet_20.fit(x_train, y_train)
elasticnet_pred_20 = elsticnet_20.predict(x_test)
```
#### PipeLine

이제 학습의 과정이

전처리 -> fit -> predict -> score 인것을 알 수 있다.

이러한 일련의 과정을 하나의 통로로 정리한 것이 PipeLine이다

```python
from sklearn.pipeline import make_pipeline
pipeline = make_pipeline(
    MinMaxScaler(), 
    ElasticNet(alpha=0.1, l1_ratio=0.2),
)

pipeline.fit(x_train, y_train)
pipeline_pred = pipeline.predict(x_test)
```

## Polynomial Features

다항 회귀... 선형회귀로는 해결할 수 없는 문제를 다루기 위해 사용.

데이터 셋이 왼편의 그림처럼 비선형성을 부분적으로 보인다면?

-> 단순회귀 방식을 이용해서 성능 향상을 꾀하기가 힘들다.

데이터 셋이 비선형적인 부분이 있다면 다항회귀를 적용해야만 성능을 올릴 수 있다.

### 컬럼을 활용하기

타이타닉 데이터 셋에서는 sibling과 parchild 컬럼을 통해 family라는 컬럼을 창출 할 수 있다.

하지만 반도체 데이터를 본다면 의미를 알 수 없는 컬럼이 많기 때문에 의미를 연결해서 새로운 컬럼을 창출할 수 없다.

### 다항 회귀 특성 추가

1. 단순히 컬럼간의 곱으로 새로운 컬럼을 만든다
2. 단순히 컬럼의 제곱으로 새로운 컬럼을 만든다 ...이렇게 컬럼 수 늘림

> 연산 방식에서 더하기와 빼기는 왜 하지 않을까?

키 180cm + 시력 1.7 -> 의미가 없음

    결론

    컬럼간의 연산, 병합 같은 일련의 행위를 통해서 "종합적인 정보를 획득"할 수 있는 장점이 있다.

다항회귀 파이프라인 예시코드
```python
poly_pipeline = make_pipeline(
    StandardScaler(),
    PolynomialFeatures(degree=2, include_bias=False),
    ElasticNet(alpha=0.1, l1_ratio=0.2),
)
```