---
layout: post
title: "머신러닝 1"
subtitle: 머신러닝
categories: machine learning
tags: [ml]
---
# 기계학습의 개요

## sklearn의 개요를 알자
머신러닝을 배우는 가장 쉬운 방법 

    ➡️ sklearn의 라이브러리를 알아보는 것이다.
아나콘다를 활용하자
    ➡️ 모든 라이브러리가 내장되어있다.

## 환경설정

anaconda설치 파일을 관리자모드로 실행해 설치를 한다.

    ➡️ 경로를 지정할 수 있기 때문
![useful](/assets/img/0915/useful.png)

just me -> default setting을 설정하고 설치

### 주피터 노트북 경로 설정

대상 설정 맨 오른쪽에 %USER를 지우고 원하는 디렉토리 경로를 명시해주면 주피터 노트북을 열었을 때 바로 그 경로로 갈 수 있다.

## 사용할 도메인

```
numpy, pandas, 머신러닝 라이브러리, 데이터타입, 변수...
```
### 데이터 분석의 전략
1. 결측치 제거 전략

2. 이상치 제거 전략

3. 제거대상 Feature를 추출

4. Feature들의 병합을 통해 새로운 컬럼 창출

5. 구획화(Binning 기법)


    TMI : bin은 binning으로 구역을 정하는 변수로도 쓰이고 binary라는 실행파일의 의미도 가지고 있다.

> 데이터 분석을 통해서 조직, 대상의 INSIGHT를 확보가 가능

### 결국은 시각화
데이터 분석의 결과를 대부분 비개발자가 확인한다.

그러므로 이해를 위해 시각화가 매우 중요하다.

### 결국은 전처리

1. Encoding 기법 : 문자를 숫자로 변경하기

2. Scaling 기법 : 표준화, 정규화

왜? -> 기계==Model==(통계학적)알고리즘(함수)이기 때문

ex) KNN, 결정트리, 회귀

> 데이터가 좋아야 학습이 잘된다.

아서 사무엘의 머신러닝 정의
> "컴퓨터가 명시적으로 프로그램되지 않고도 학습할 수 있도록 하는 연구 분야"

### 지도/ 비지도 학습

Input Data, Label < Training DataSet >

데이터를 입력하는 것 만큼 데이터를 나누는 것도 중요하다.

![ima](/assets/img/0915/example.png)

Feature는 속성이고 Target은 정답이라고 볼 수 있다.
> Feature = 속성 = x = Training Data

> Target = 정답 = y = label = class = Test Data

> 학습하다 = Fit

> 결과를 확인하기 = Predict

이와 같이 라벨이 되어있는 학습을 지도 학습이라고 한다.

학습데이터, 테스트데이터를 구분해서 사용하자

정확도와 Error, Loss는 반비례한다.

### 모델의 종류

1. Train model

2. Test model

![test](/assets/img/0915/2.png)

# 머신러닝 기초

## Dataset 다루기
sklearn.dataset 안에는 빌트인 (built-in) 데이터 셋들이 존재한다.

- `load_boston`: 보스톤 집값 데이터
- `load_iris`: 아이리스 붓꽃 데이터
- `load_diabetes`: 당뇨병 환자 데이터
- `load_digits`: 손글씨 데이터
- `load_linnerud`: multi-output regression 용 데이터
- `load_wine`: 와인 데이터
- `load_breast_cancer`: 위스콘신 유방암 환자 데이터

데이터는 csv로 로컬에 저장되어있다.

**key-value** 형식으로 구성되어 있으며, 사전(dict)형 타입과 유사한 구조를 가짐.

공통 **key**는 다음과 같다.

- **`data`**: 샘플 데이터, Numpy 배열.
- **`target`**: Label 데이터, Numpy 배열.
- **`feature_names`**: Feature 데이터의 이름
- **`target_names`**: Label 데이터의 이름
- `DESCR`: 데이터 셋의 설명
- `filename`: 데이터 셋의 파일 저장 위치 (csv)

**data, target, target_names, feature_names** 
4가지 key값은 반드시 기억!

필요 모델 로드
```python
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np

iris = load_iris()
iris.keys()
```
결과
```
dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])
```
load한 데이터는 dictionary이고 key로 데이터에 접근할 수 있다.

iris.data를 dataframe으로 바꾸어 보았다.

![hello](/assets/img/0915/tester.png)

key접근을 아래로 할 수 있다.

```python
iris['DESCR']
iris.DESCR
```
numpy, dataframe으로 접근

```python
iris_np = np.array(iris.data)
iris_df = pd.DataFrame(iris['data'], columns=iris['feature_names'])   
```

```python
# 데이터프레임화 
iris_df = pd.DataFrame(iris['data'],columns = iris['feature_names'])
iris_df['target'] = iris['target']

# 시각화
plt.figure(figsize=(10,7))

# 컬럼중 sepal만
# palette -> spring, spring_r _r로 색깔을 거꾸로 표현가능
sns.scatterplot(iris_df.iloc[:, 0], iris_df.iloc[:,1], hue=iris_df['target'], palette='spring').set_title("Sepal", fontsize=17)
```

## 데이터 구분하기

```python
from sklearn.model_selection import train_test_split
```
**주요 hyperparameter**

* `test_size`: Test(validation) Set에 할당할 비율 (20% -> 0.2), 기본값 0.25
* `stratify`: 분할된 샘플의 class 갯수가 동일한 비율로 유지
* `random_state`: 랜덤 시드값
* `shuffle`: 셔플 옵션, 기본값 True
```python
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=321, test_size=0.2, stratify=iris.target)
# stratify = 층화추출법 - class를 정해준다.
display(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
```

## KNN 알고리즘

K개의 이웃의 정보로 데이터를 예측하는 알고리즘
![useful2](/assets/img/0915/useful2.png)

짝수 - 가장 가까운 이웃이 target

홀수 - 갯수가 더 많은 쪽

`거리를 계산하는 방법은 유클리드 거리계산을 이용`

```python
from sklearn.neighbors import KNeighborsClassifier
# mnist (손글씨) 데이터셋
from tensorflow.keras.datasets import mnist
x_train, x_test, y_train, y_test = train_test_split(x_digit, y_digit, random_state=30, stratify=y_digit, test_size=0.1)

# n_jobs 옵션은 사용하고 있는 machine의 모든 core 개수
# default가 5개
knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
```
학습하고 정확도 확인하기
```python
    # 학습하기
    knn.fit(x_train, y_train)
    # 결과 예측
    prediction = knn.predict(x_test)
    # 정확도 계산
    (prediction == y_test).mean()

# knn.score를 통한 계산
knn.score(x_test, y_test)
# prediction을 내부적으로 계산하고 y_test랑 비교해 결과를 리턴한다.
```
